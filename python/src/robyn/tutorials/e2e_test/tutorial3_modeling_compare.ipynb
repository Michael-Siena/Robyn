{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robyn: Marketing Mix Modeling Application\n",
    "\n",
    "This notebook demonstrates the usage of Robyn, a Marketing Mix Modeling (MMM) application. \n",
    "We'll go through the main steps of performing robyn_inputs and robyn_engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Import Required Libraries. Define Paths.\n",
    "\n",
    "First, be sure to setup your virtual environment. Be sure to switch over to your new environment in this notebook. \n",
    "\n",
    "-```cd {root_folder}```\n",
    "\n",
    "-```python3 -m yourvenv```\n",
    "\n",
    "-```source yourvenv/bin/activate```\n",
    "\n",
    "-```cd Robyn/python```\n",
    "\n",
    "-```pip install -r requirements.txt```\n",
    "\n",
    "\n",
    "Then import the necessary libraries. Make sure to define your paths below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add Robyn to path\n",
    "sys.path.append(\"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from robyn.data.entities.mmmdata import MMMData\n",
    "from robyn.data.entities.enums import AdstockType\n",
    "from robyn.data.entities.holidays_data import HolidaysData\n",
    "from robyn.data.entities.hyperparameters import Hyperparameters, ChannelHyperparameters\n",
    "from robyn.modeling.entities.modelrun_trials_config import TrialsConfig\n",
    "from robyn.modeling.model_executor import ModelExecutor\n",
    "from robyn.modeling.entities.enums import NevergradAlgorithm, Models\n",
    "from robyn.modeling.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Mock R data\n",
    "\n",
    "We need to set the base path for the data directory.\n",
    "Create a .env file in the same directory as your notebook and put in define the path to the data dir.\n",
    "for example: ROBYN_BASE_PATH=.../Robyn/R/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the simulated data and holidays data\n",
    "dt_simulated_weekly = pd.read_csv(\n",
    "    \"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src/robyn/tutorials/resources/dt_simulated_weekly.csv\"\n",
    ")\n",
    "\n",
    "dt_prophet_holidays = pd.read_csv(\n",
    "    \"/Users/yijuilee/robynpy_release_reviews/Robyn/python/src/robyn/tutorials/resources/dt_prophet_holidays.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MMM Data\n",
    "\n",
    "We will now set up the MMM data specification which includes defining the dependent variable, independent variables, and the time window for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mmm_data(dt_simulated_weekly) -> MMMData:\n",
    "\n",
    "    mmm_data_spec = MMMData.MMMDataSpec(\n",
    "        dep_var=\"revenue\",\n",
    "        dep_var_type=\"revenue\",\n",
    "        date_var=\"DATE\",\n",
    "        context_vars=[\"competitor_sales_B\", \"events\"],\n",
    "        paid_media_spends=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_S\", \"search_S\"],\n",
    "        paid_media_vars=[\"tv_S\", \"ooh_S\", \"print_S\", \"facebook_I\", \"search_clicks_P\"],\n",
    "        organic_vars=[\"newsletter\"],\n",
    "        window_start=\"2016-01-01\",\n",
    "        window_end=\"2018-12-31\",\n",
    "    )\n",
    "\n",
    "    return MMMData(data=dt_simulated_weekly, mmmdata_spec=mmm_data_spec)\n",
    "\n",
    "\n",
    "mmm_data = setup_mmm_data(dt_simulated_weekly)\n",
    "mmm_data.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "We will perform feature engineering to prepare the data for modeling. This includes transformations like adstock and other preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Hyperparameters(\n",
    "    {\n",
    "        \"facebook_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"print_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"tv_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.3, 0.8],\n",
    "        ),\n",
    "        \"search_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0, 0.3],\n",
    "        ),\n",
    "        \"ooh_S\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "        \"newsletter\": ChannelHyperparameters(\n",
    "            alphas=[0.5, 3],\n",
    "            gammas=[0.3, 1],\n",
    "            thetas=[0.1, 0.4],\n",
    "        ),\n",
    "    },\n",
    "    adstock=AdstockType.GEOMETRIC,\n",
    "    lambda_=[0, 1],\n",
    "    train_size=[0.5, 0.8],\n",
    ")\n",
    "\n",
    "print(\"Hyperparameters setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HolidaysData object\n",
    "holidays_data = HolidaysData(\n",
    "    dt_holidays=dt_prophet_holidays,\n",
    "    prophet_vars=[\"trend\", \"season\", \"holiday\"],\n",
    "    prophet_country=\"DE\",\n",
    "    prophet_signs=[\"default\", \"default\", \"default\"],\n",
    ")\n",
    "# Setup FeaturizedMMMData\n",
    "feature_engineering = FeatureEngineering(mmm_data, hyperparameters, holidays_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_mmm_data = feature_engineering.perform_feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.visualization.feature_visualization import FeaturePlotter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a FeaturePlotter instance\n",
    "feature_plotter = FeaturePlotter(mmm_data, hyperparameters)\n",
    "# Extract the list of results\n",
    "results_list = featurized_mmm_data.modNLS[\"results\"]\n",
    "# Plot spend-exposure relationship for each channel in the results\n",
    "for result in results_list:\n",
    "    channel = result[\"channel\"]\n",
    "    try:\n",
    "        fig = feature_plotter.plot_spend_exposure(featurized_mmm_data, channel)\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping {channel}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ModelExecutor\n",
    "model_executor = ModelExecutor(\n",
    "    mmmdata=mmm_data,\n",
    "    holidays_data=holidays_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    calibration_input=None,  # Add calibration input if available\n",
    "    featurized_mmm_data=featurized_mmm_data,\n",
    ")\n",
    "\n",
    "# Setup TrialsConfig\n",
    "trials_config = TrialsConfig(iterations=54, trials=5)  # Set to the number of cores you want to use\n",
    "\n",
    "print(\n",
    "    f\">>> Starting {trials_config.trials} trials with {trials_config.iterations} iterations each using {NevergradAlgorithm.TWO_POINTS_DE.value} nevergrad algorithm on x cores...\"\n",
    ")\n",
    "\n",
    "# Run the model\n",
    "\n",
    "output_models = model_executor.model_run(\n",
    "    trials_config=trials_config,\n",
    "    ts_validation=True,  # changed from True to False -> deacitvate\n",
    "    add_penalty_factor=False,\n",
    "    rssd_zero_penalty=True,\n",
    "    cores=8,\n",
    "    nevergrad_algo=NevergradAlgorithm.TWO_POINTS_DE,\n",
    "    intercept=True,\n",
    "    intercept_sign=\"non_negative\",\n",
    "    model_name=Models.RIDGE,\n",
    ")\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from robyn.visualization.model_convergence_visualizer import ModelConvergenceVisualizer\n",
    "\n",
    "model_visualizer = ModelConvergenceVisualizer(\n",
    "    moo_distrb_plot=output_models.convergence[\"moo_distrb_plot\"],\n",
    "    ts_validation_plot=output_models.convergence[\"ts_validation_plot\"],\n",
    "    moo_cloud_plot=output_models.convergence[\"moo_cloud_plot\"],\n",
    ")\n",
    "\n",
    "# 1. Display the MOO Distribution Plot\n",
    "if \"moo_distrb_plot\" in output_models.convergence:\n",
    "    model_visualizer.display_moo_distrb_plot()\n",
    "\n",
    "# 2. Display the MOO Cloud Plot\n",
    "if \"moo_cloud_plot\" in output_models.convergence:\n",
    "    model_visualizer.display_moo_cloud_plot()\n",
    "\n",
    "# 3. Display time series validation and convergence plots\n",
    "if \"ts_validation_plot\" in output_models.convergence:\n",
    "    model_visualizer.display_ts_validation_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robyn.tutorials.utils.data_mapper import load_data_from_json, import_input_collect, import_output_models\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load data from JSON exported from R\n",
    "raw_input_collect = load_data_from_json(\n",
    "    \"/Users/yijuilee/project_robyn/original/Robyn_original_2/Robyn/robyn_api/data/Pareto_50_InputCollect.json\"\n",
    ")\n",
    "raw_output_models = load_data_from_json(\n",
    "    \"/Users/yijuilee/project_robyn/original/Robyn_original_2/Robyn/robyn_api/data/Pareto_50_OutputModels.json\"\n",
    ")\n",
    "\n",
    "# Convert R data to Python objects\n",
    "r_input_collect = import_input_collect(raw_input_collect)\n",
    "r_output_models = import_output_models(raw_output_models)\n",
    "\n",
    "# Extract individual components\n",
    "r_mmm_data = r_input_collect[\"mmm_data\"]\n",
    "r_featurized_mmm_data = r_input_collect[\"featurized_mmm_data\"]\n",
    "r_holidays_data = r_input_collect[\"holidays_data\"]\n",
    "r_hyperparameters = r_input_collect[\"hyperparameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anytree import Node, RenderTree\n",
    "from dataclasses import is_dataclass, asdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_tree(data, parent_key=\"\", limit_trials=False, limit_repeats=False):\n",
    "    \"\"\"\n",
    "    Recursively build a tree structure from a dictionary, list, or dataclass.\n",
    "\n",
    "    Args:\n",
    "        data: The data structure (dict, list, or dataclass) to traverse.\n",
    "        parent_key: The base key path for nested keys.\n",
    "        limit_trials: Whether to limit the output to the first trial.\n",
    "        limit_repeats: Whether to limit the output to the first occurrence of repeated keys.\n",
    "\n",
    "    Returns:\n",
    "        A tree node representing the structure of the data.\n",
    "    \"\"\"\n",
    "    if is_dataclass(data):\n",
    "        data = asdict(data)  # Convert dataclass to dictionary\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        node = Node(f\"{parent_key} (dict)\")\n",
    "        for key, value in data.items():\n",
    "            full_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "            child_node = build_tree(value, full_key, limit_trials, limit_repeats)\n",
    "            child_node.parent = node\n",
    "        return node\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        node = Node(f\"{parent_key} (list, length={len(data)})\")\n",
    "        item_structure = None\n",
    "        repeat_count = 0\n",
    "        for index, item in enumerate(data):\n",
    "            full_key = f\"{parent_key}[{index}]\"\n",
    "            if limit_trials and parent_key == \"trials\" and index > 0:\n",
    "                break\n",
    "\n",
    "            # Serialize the structure of the item for comparison\n",
    "            current_structure = (\n",
    "                str({k: type(v).__name__ for k, v in item.items()}) if isinstance(item, dict) else str(type(item))\n",
    "            )\n",
    "\n",
    "            if item_structure is None:\n",
    "                item_structure = current_structure\n",
    "                child_node = build_tree(item, full_key, limit_trials, limit_repeats)\n",
    "                child_node.parent = node\n",
    "            elif current_structure == item_structure:\n",
    "                repeat_count += 1\n",
    "            else:\n",
    "                # If the structure changes, reset the counter and process the new structure\n",
    "                if repeat_count > 0:\n",
    "                    summary_node = Node(f\"{parent_key} (repeats for {repeat_count} times)\")\n",
    "                    summary_node.parent = node\n",
    "                item_structure = current_structure\n",
    "                repeat_count = 0\n",
    "                child_node = build_tree(item, full_key, limit_trials, limit_repeats)\n",
    "                child_node.parent = node\n",
    "\n",
    "        # Add a summary node if there are repeated structures at the end\n",
    "        if repeat_count > 0:\n",
    "            summary_node = Node(f\"{parent_key} (repeats for {repeat_count} times)\")\n",
    "            summary_node.parent = node\n",
    "\n",
    "        return node\n",
    "\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        node = Node(f\"{parent_key} (DataFrame: {data.shape})\")\n",
    "        for column in data.columns:\n",
    "            dtype = data[column].dtype\n",
    "            column_node = Node(f\"{parent_key}.{column} (dtype: {dtype})\")\n",
    "            column_node.parent = node\n",
    "        return node\n",
    "\n",
    "    else:\n",
    "        dtype = type(data).__name__\n",
    "        return Node(f\"{parent_key} (type: {dtype})\")\n",
    "\n",
    "\n",
    "# Assuming featurized_mmm_data is an instance of FeaturizedMMMData\n",
    "python_tree = build_tree(output_models)\n",
    "r_tree = build_tree(r_output_models)\n",
    "\n",
    "# Visualize the tree\n",
    "print(\"Python ModelOutputs Structure:\")\n",
    "for pre, fill, node in RenderTree(python_tree):\n",
    "    print(f\"{pre}{node.name}\")\n",
    "\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"R ModelOutputs Structure:\")\n",
    "for pre, fill, node in RenderTree(python_tree):\n",
    "    print(f\"{pre}{node.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from anytree import Node, RenderTree\n",
    "# from anytree.exporter import DotExporter\n",
    "# from dataclasses import is_dataclass, asdict\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def build_tree(data, parent_key=\"\", limit_trials=True):\n",
    "#     \"\"\"\n",
    "#     Recursively build a tree structure from a dictionary, list, or dataclass.\n",
    "\n",
    "#     Args:\n",
    "#         data: The data structure (dict, list, or dataclass) to traverse.\n",
    "#         parent_key: The base key path for nested keys.\n",
    "#         limit_trials: Whether to limit the output to the first trial.\n",
    "\n",
    "#     Returns:\n",
    "#         A tree node representing the structure of the data.\n",
    "#     \"\"\"\n",
    "#     if is_dataclass(data):\n",
    "#         data = asdict(data)  # Convert dataclass to dictionary\n",
    "\n",
    "#     if isinstance(data, dict):\n",
    "#         node = Node(parent_key)\n",
    "#         for key, value in data.items():\n",
    "#             full_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "#             child_node = build_tree(value, full_key, limit_trials)\n",
    "#             child_node.parent = node\n",
    "#         return node\n",
    "#     elif isinstance(data, list):\n",
    "#         node = Node(parent_key)\n",
    "#         for index, item in enumerate(data):\n",
    "#             if limit_trials and parent_key == \"trials\" and index > 0:\n",
    "#                 break\n",
    "#             full_key = f\"{parent_key}[{index}]\"\n",
    "#             child_node = build_tree(item, full_key, limit_trials)\n",
    "#             child_node.parent = node\n",
    "#         return node\n",
    "#     elif isinstance(data, pd.DataFrame):\n",
    "#         node = Node(f\"{parent_key} (DataFrame: {data.shape})\")\n",
    "#         for column in data.columns:\n",
    "#             column_node = Node(f\"{parent_key}.{column}\")\n",
    "#             column_node.parent = node\n",
    "#         return node\n",
    "#     else:\n",
    "#         return Node(parent_key)\n",
    "\n",
    "\n",
    "# # Assuming output_models and r_output_models are instances of ModelOutputs\n",
    "# python_tree = build_tree(output_models)\n",
    "# r_tree = build_tree(r_output_models)\n",
    "\n",
    "# # Visualize the trees\n",
    "# print(\"Python ModelOutputs Structure:\")\n",
    "# for pre, fill, node in RenderTree(python_tree):\n",
    "#     print(f\"{pre}{node.name}\")\n",
    "\n",
    "# print(\"\\nR ModelOutputs Structure:\")\n",
    "# for pre, fill, node in RenderTree(r_tree):\n",
    "#     print(f\"{pre}{node.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compare_model_values(py_output, r_output):\n",
    "    \"\"\"Compare key values between Python and R model outputs\"\"\"\n",
    "\n",
    "    print(\"1. Basic Model Configuration Comparison:\")\n",
    "    basic_attrs = [\n",
    "        \"train_timestamp\",\n",
    "        \"cores\",\n",
    "        \"iterations\",\n",
    "        \"intercept\",\n",
    "        \"intercept_sign\",\n",
    "        \"nevergrad_algo\",\n",
    "        \"ts_validation\",\n",
    "        \"add_penalty_factor\",\n",
    "    ]\n",
    "\n",
    "    # Add debug prints\n",
    "    print(\"\\nDebugging attribute types:\")\n",
    "    for attr in basic_attrs:\n",
    "        py_val = getattr(py_output, attr, None)\n",
    "        r_val = getattr(r_output, attr, None)\n",
    "        print(f\"{attr:20s} - Python type: {type(py_val)} | R type: {type(r_val)}\")\n",
    "        print(f\"{attr:20s} - Python value: {py_val} | R value: {r_val}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\n2. Trial Results Comparison (Descriptive Statistics):\")\n",
    "    if py_output.trials and r_output.trials:\n",
    "        metrics = [\n",
    "            \"nrmse\",\n",
    "            \"decomp_rssd\",\n",
    "            \"mape\",\n",
    "            \"rsq_train\",\n",
    "            \"rsq_val\",\n",
    "            \"rsq_test\",\n",
    "            \"lambda_\",\n",
    "            \"lambda_hp\",\n",
    "            \"lambda_max\",\n",
    "            \"lambda_min_ratio\",\n",
    "        ]\n",
    "        # Convert trial results to DataFrames\n",
    "        py_trials_df = pd.DataFrame(\n",
    "            [{metric: getattr(trial, metric, np.nan) for metric in metrics} for trial in py_output.trials]\n",
    "        )\n",
    "\n",
    "        # Aggregate R trial metrics\n",
    "        r_trials_df = pd.DataFrame(\n",
    "            [\n",
    "                {metric: getattr(trial, metric, pd.Series([np.nan])).mean() for metric in metrics}\n",
    "                for trial in r_output.trials\n",
    "            ]\n",
    "        )\n",
    "        # Ensure R trial data is numeric\n",
    "        r_trials_df = r_trials_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        # Calculate descriptive statistics\n",
    "        py_desc = py_trials_df.describe()\n",
    "        r_desc = r_trials_df.describe()\n",
    "        # Print descriptive statistics\n",
    "        print(\"\\nPython Trial Descriptive Statistics:\")\n",
    "        print(py_desc)\n",
    "        print(\"\\nR Trial Descriptive Statistics:\")\n",
    "        print(r_desc)\n",
    "        # Calculate and print differences\n",
    "        diff_desc = py_desc - r_desc\n",
    "        print(\"\\nDifference in Descriptive Statistics:\")\n",
    "        print(diff_desc)\n",
    "\n",
    "    print(\"\\n3. Hyperparameter Comparison:\")\n",
    "    if hasattr(py_output, \"hyper_updated\") and hasattr(r_output, \"hyper_updated\"):\n",
    "        py_hyper = py_output.hyper_updated\n",
    "        r_hyper = r_output.hyper_updated\n",
    "\n",
    "        # Find all unique keys\n",
    "        all_keys = set(py_hyper.keys()) | set(r_hyper.keys())\n",
    "\n",
    "        print(\"\\nHyperparameter Values:\")\n",
    "        print(f\"{'Parameter':30s} {'Python':>15s} {'R':>15s} {'Diff':>15s}\")\n",
    "        print(\"-\" * 75)\n",
    "\n",
    "        for key in sorted(all_keys):\n",
    "            py_val = py_hyper.get(key, \"N/A\")\n",
    "            r_val = r_hyper.get(key, \"N/A\")\n",
    "\n",
    "            if isinstance(py_val, (int, float)) and isinstance(r_val, (int, float)):\n",
    "                diff = abs(py_val - r_val)\n",
    "                print(f\"{key:30s} {py_val:15.6f} {r_val:15.6f} {diff:15.6f}\")\n",
    "            else:\n",
    "                print(f\"{key:30s} {str(py_val):15s} {str(r_val):15s} {'N/A':>15s}\")\n",
    "\n",
    "    print(\"\\n4. Data Shape Comparison:\")\n",
    "    data_attrs = [\"all_result_hyp_param\", \"all_x_decomp_agg\", \"all_decomp_spend_dist\"]\n",
    "\n",
    "    for attr in data_attrs:\n",
    "        py_shape = getattr(py_output, attr).shape if hasattr(py_output, attr) else None\n",
    "        r_shape = getattr(r_output, attr).shape if hasattr(r_output, attr) else None\n",
    "        print(f\"{attr:20s} - Python shape: {py_shape} | R shape: {r_shape}\")\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "print(\"Starting detailed value comparison...\\n\")\n",
    "compare_model_values(output_models, r_output_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "pd.set_option(\"display.width\", 1000)  # Set the display width\n",
    "\n",
    "\n",
    "def get_param_dict(param):\n",
    "    \"\"\"Helper function to safely get parameter dictionary\"\"\"\n",
    "    if hasattr(param, \"__dict__\"):\n",
    "        return vars(param)\n",
    "    elif isinstance(param, dict):\n",
    "        return param\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def compare_hyperparameters(py_hyp, r_hyp):\n",
    "    \"\"\"Compare Hyperparameters with value previews - handles both dict and object structures\"\"\"\n",
    "    print(\"\\n=== Hyperparameters Comparison ===\")\n",
    "    print(\"\\nAdstock type:\")\n",
    "    print(f\"Python: {py_hyp.adstock}\")\n",
    "    print(f\"R: {r_hyp.adstock}\")\n",
    "\n",
    "    # Compare hyperparameters for each channel with values\n",
    "    print(\"\\nChannel hyperparameters:\")\n",
    "\n",
    "    # Safely get channel lists\n",
    "    py_channels = (\n",
    "        py_hyp.hyperparameters.keys() if hasattr(py_hyp, \"hyperparameters\") and py_hyp.hyperparameters else []\n",
    "    )\n",
    "    r_channels = r_hyp.hyperparameters.keys() if hasattr(r_hyp, \"hyperparameters\") and r_hyp.hyperparameters else []\n",
    "\n",
    "    # Print structure debug info\n",
    "    print(\"\\nDebug - Hyperparameters structure:\")\n",
    "    print(f\"Python hyperparameters type: {type(py_hyp.hyperparameters)}\")\n",
    "    print(f\"R hyperparameters type: {type(r_hyp.hyperparameters)}\")\n",
    "\n",
    "    for channel in set(py_channels) | set(r_channels):\n",
    "        print(f\"\\nChannel: {channel}\")\n",
    "\n",
    "        # Python parameters\n",
    "        if channel in py_channels:\n",
    "            py_params = py_hyp.hyperparameters[channel]\n",
    "            print(\"Python params:\")\n",
    "            param_dict = get_param_dict(py_params)\n",
    "            for param_name, param_value in param_dict.items():\n",
    "                if param_value is not None:\n",
    "                    if isinstance(param_value, (list, tuple)):\n",
    "                        print(f\"{param_name}: {param_value[0] if param_value else None}\")\n",
    "                    else:\n",
    "                        print(f\"{param_name}: {param_value}\")\n",
    "\n",
    "        # R parameters\n",
    "        if channel in r_channels:\n",
    "            r_params = r_hyp.hyperparameters[channel]\n",
    "            print(\"R params:\")\n",
    "            param_dict = get_param_dict(r_params)\n",
    "            for param_name, param_value in param_dict.items():\n",
    "                if param_value is not None:\n",
    "                    if isinstance(param_value, (list, tuple)):\n",
    "                        print(f\"{param_name}: {param_value[0] if param_value else None}\")\n",
    "                    else:\n",
    "                        print(f\"{param_name}: {param_value}\")\n",
    "\n",
    "\n",
    "def describe_df_columns(df, prefix=\"\"):\n",
    "    \"\"\"Helper function to describe each column in a DataFrame\"\"\"\n",
    "    print(f\"\\n{prefix} Columns Summary:\")\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            if df[col].dtype.kind in \"biufc\":  # boolean, integer, unsigned integer, float, complex\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(df[col].describe())\n",
    "        except:\n",
    "            print(f\"\\nCould not generate statistics for {col}\")\n",
    "\n",
    "\n",
    "def compare_mmm_data(py_mmm_data, r_mmm_data):\n",
    "    \"\"\"Compare MMMData components with value previews\"\"\"\n",
    "    print(\"\\n=== MMMData Comparison ===\")\n",
    "\n",
    "    # Compare main data values\n",
    "    print(\"\\nMain data value comparison:\")\n",
    "    print(\"\\nPython data summary:\")\n",
    "    describe_df_columns(py_mmm_data.data, \"Python\")\n",
    "    print(\"\\nR data summary:\")\n",
    "    describe_df_columns(r_mmm_data.data, \"R\")\n",
    "\n",
    "    # Compare MMMDataSpec attributes\n",
    "    print(\"\\nMMMDataSpec attributes:\")\n",
    "    spec_attrs = [\n",
    "        \"all_media\",\n",
    "        \"paid_media_spends\",\n",
    "        \"context_vars\",\n",
    "        \"dep_var_type\",\n",
    "        \"rolling_window_start_which\",\n",
    "        \"rolling_window_end_which\",\n",
    "        \"rolling_window_length\",\n",
    "        \"window_start\",\n",
    "        \"window_end\",\n",
    "    ]\n",
    "\n",
    "    for attr in spec_attrs:\n",
    "        print(f\"\\n{attr}:\")\n",
    "        print(f\"Python: {getattr(py_mmm_data.mmmdata_spec, attr)}\")\n",
    "        print(f\"R: {getattr(r_mmm_data.mmmdata_spec, attr)}\")\n",
    "\n",
    "\n",
    "def compare_featurized_data(py_feat, r_feat):\n",
    "    \"\"\"Compare FeaturizedMMMData components with value previews\"\"\"\n",
    "    print(\"\\n=== FeaturizedMMMData Comparison ===\")\n",
    "\n",
    "    # Compare dt_mod values\n",
    "    print(\"\\ndt_mod value comparison:\")\n",
    "    print(\"\\nPython dt_mod summary:\")\n",
    "    describe_df_columns(py_feat.dt_mod, \"Python dt_mod\")\n",
    "    print(\"\\nR dt_mod summary:\")\n",
    "    describe_df_columns(r_feat.dt_mod, \"R dt_mod\")\n",
    "\n",
    "    # Compare dt_modRollWind values\n",
    "    print(\"\\ndt_modRollWind value comparison:\")\n",
    "    print(\"\\nPython dt_modRollWind summary:\")\n",
    "    describe_df_columns(py_feat.dt_modRollWind, \"Python dt_modRollWind\")\n",
    "    print(\"\\nR dt_modRollWind summary:\")\n",
    "    describe_df_columns(r_feat.dt_modRollWind, \"R dt_modRollWind\")\n",
    "\n",
    "\n",
    "# def compare_model_outputs(py_model, r_model):\n",
    "#     \"\"\"Compare ModelOutputs with value previews\"\"\"\n",
    "#     print(\"\\n=== ModelOutputs Comparison ===\")\n",
    "\n",
    "#     # Compare basic attributes\n",
    "#     attrs = [\"cores\", \"iterations\", \"hyper_fixed\", \"ts_validation\"]\n",
    "#     for attr in attrs:\n",
    "#         if hasattr(py_model, attr) and hasattr(r_model, attr):\n",
    "#             print(f\"\\n{attr}:\")\n",
    "#             print(f\"Python: {getattr(py_model, attr)}\")\n",
    "#             print(f\"R: {getattr(r_model, attr)}\")\n",
    "\n",
    "#     if py_model.trials and r_model.trials:\n",
    "#         print(\"\\n=== Last Trial Comparison ===\")\n",
    "\n",
    "#         # Compare result_hyp_param\n",
    "#         print(\"\\nResult Hyperparameter Values:\")\n",
    "#         if hasattr(py_model.trials[4], \"result_hyp_param\"):\n",
    "#             print(\"\\nPython result_hyp_param summary:\")\n",
    "#             describe_df_columns(py_model.trials[4].result_hyp_param, \"Python\")\n",
    "#         if hasattr(r_model.trials[4], \"result_hyp_param\"):\n",
    "#             print(\"\\nR result_hyp_param summary:\")\n",
    "#             describe_df_columns(r_model.trials[4].result_hyp_param, \"R\")\n",
    "\n",
    "#         # Compare x_decomp_agg\n",
    "#         print(\"\\nX Decomp Agg Values:\")\n",
    "#         if hasattr(py_model.trials[4], \"x_decomp_agg\"):\n",
    "#             print(\"\\nPython x_decomp_agg summary:\")\n",
    "#             describe_df_columns(py_model.trials[4].x_decomp_agg, \"Python\")\n",
    "#         if hasattr(r_model.trials[4], \"x_decomp_agg\"):\n",
    "#             print(\"\\nR x_decomp_agg summary:\")\n",
    "#             describe_df_columns(r_model.trials[4].x_decomp_agg, \"R\")\n",
    "\n",
    "#         # Compare decomp_spend_dist if available\n",
    "#         if hasattr(py_model.trials[4], \"decomp_spend_dist\") and hasattr(r_model.trials[4], \"decomp_spend_dist\"):\n",
    "#             print(\"\\nDecomp Spend Distribution Values:\")\n",
    "#             print(\"\\nPython decomp_spend_dist summary:\")\n",
    "#             describe_df_columns(py_model.trials[4].decomp_spend_dist, \"Python\")\n",
    "#             print(\"\\nR decomp_spend_dist summary:\")\n",
    "#             describe_df_columns(r_model.trials[4].decomp_spend_dist, \"R\")\n",
    "\n",
    "\n",
    "def compare_model_outputs(py_model, r_model):\n",
    "    \"\"\"Compare ModelOutputs with value previews\"\"\"\n",
    "    print(\"\\n=== ModelOutputs Comparison ===\")\n",
    "    # Compare basic attributes\n",
    "    attrs = [\"cores\", \"iterations\", \"hyper_fixed\", \"ts_validation\"]\n",
    "    for attr in attrs:\n",
    "        if hasattr(py_model, attr) and hasattr(r_model, attr):\n",
    "            print(f\"\\n{attr}:\")\n",
    "            print(f\"Python: {getattr(py_model, attr)}\")\n",
    "            print(f\"R: {getattr(r_model, attr)}\")\n",
    "    if py_model.trials and r_model.trials:\n",
    "        print(\"\\n=== Last Trial Comparison ===\")\n",
    "\n",
    "        # Helper function to describe and stack dataframes\n",
    "        def describe_and_stack(py_df, r_df, label):\n",
    "            if py_df is not None and r_df is not None:\n",
    "                py_desc = py_df.describe()\n",
    "                r_desc = r_df.describe()\n",
    "                combined = pd.concat([py_desc, r_desc], axis=1, keys=[\"Python\", \"R\"])\n",
    "                print(f\"\\n{label} summary:\")\n",
    "                print(combined)\n",
    "\n",
    "        # Compare result_hyp_param\n",
    "        if hasattr(py_model.trials[4], \"result_hyp_param\") and hasattr(r_model.trials[4], \"result_hyp_param\"):\n",
    "            describe_and_stack(\n",
    "                py_model.trials[4].result_hyp_param, r_model.trials[4].result_hyp_param, \"Result Hyperparameter Values\"\n",
    "            )\n",
    "        # Compare x_decomp_agg\n",
    "        if hasattr(py_model.trials[4], \"x_decomp_agg\") and hasattr(r_model.trials[4], \"x_decomp_agg\"):\n",
    "            describe_and_stack(py_model.trials[4].x_decomp_agg, r_model.trials[4].x_decomp_agg, \"X Decomp Agg Values\")\n",
    "        # Compare decomp_spend_dist if available\n",
    "        if hasattr(py_model.trials[4], \"decomp_spend_dist\") and hasattr(r_model.trials[4], \"decomp_spend_dist\"):\n",
    "            describe_and_stack(\n",
    "                py_model.trials[4].decomp_spend_dist,\n",
    "                r_model.trials[4].decomp_spend_dist,\n",
    "                \"Decomp Spend Distribution Values\",\n",
    "            )\n",
    "\n",
    "\n",
    "def run_comprehensive_comparison(py_data, r_data):\n",
    "    \"\"\"Run all comparisons with value previews\"\"\"\n",
    "    print(\"Starting comprehensive data comparison...\")\n",
    "\n",
    "    # compare_mmm_data(py_data[\"mmm_data\"], r_data[\"mmm_data\"])\n",
    "    # compare_featurized_data(py_data[\"featurized_mmm_data\"], r_data[\"featurized_mmm_data\"])\n",
    "    # compare_hyperparameters(py_data[\"hyperparameters\"], r_data[\"hyperparameters\"])\n",
    "    compare_model_outputs(py_data[\"model_outputs\"], r_data[\"model_outputs\"])\n",
    "\n",
    "\n",
    "# Your existing data loading code remains the same\n",
    "python_data = {\n",
    "    \"mmm_data\": mmm_data,\n",
    "    \"featurized_mmm_data\": featurized_mmm_data,\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"model_outputs\": output_models,\n",
    "}\n",
    "\n",
    "r_data = {\n",
    "    \"mmm_data\": r_mmm_data,\n",
    "    \"featurized_mmm_data\": r_featurized_mmm_data,\n",
    "    \"hyperparameters\": r_hyperparameters,\n",
    "    \"model_outputs\": r_output_models,\n",
    "}\n",
    "\n",
    "run_comprehensive_comparison(python_data, r_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python\")\n",
    "print(hyperparameters.hyperparameters)\n",
    "\n",
    "print(\"R\")\n",
    "print(r_input_collect[\"hyperparameters\"].hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_x_decomp_agg(py_model, r_model):\n",
    "#     \"\"\"Compare x_decomp_agg structure and values between Python and R\"\"\"\n",
    "#     print(\"\\nComparing x_decomp_agg:\")\n",
    "#     print(\"Python columns:\", py_model.all_x_decomp_agg.columns.tolist())\n",
    "#     print(\"R columns:\", r_model.all_x_decomp_agg.columns.tolist())\n",
    "\n",
    "#     # Compare common columns statistics\n",
    "#     common_cols = list(set(py_model.all_x_decomp_agg.columns) & set(r_model.all_x_decomp_agg.columns))\n",
    "#     print(\"\\nCommon columns statistics:\")\n",
    "#     for col in common_cols:\n",
    "#         py_stats = py_model.all_x_decomp_agg[col].describe()\n",
    "#         r_stats = r_model.all_x_decomp_agg[col].describe()\n",
    "#         print(f\"\\n{col}:\")\n",
    "#         print(\"Python:\", py_stats)\n",
    "#         print(\"R:\", r_stats)\n",
    "\n",
    "\n",
    "# def compare_decomp_spend_dist(py_model, r_model):\n",
    "#     \"\"\"Compare decomp_spend_dist structure and values between Python and R\"\"\"\n",
    "#     print(\"\\nComparing decomp_spend_dist:\")\n",
    "#     print(\"Python columns:\", py_model.all_decomp_spend_dist.columns.tolist())\n",
    "#     print(\"R columns:\", r_model.all_decomp_spend_dist.columns.tolist())\n",
    "\n",
    "#     # Compare value distributions for key metrics\n",
    "#     key_metrics = [\"effect_share\", \"spend_share\", \"roi_total\"]\n",
    "#     print(\"\\nKey metrics distributions:\")\n",
    "#     for metric in key_metrics:\n",
    "#         if metric in py_model.all_decomp_spend_dist.columns and metric in r_model.all_decomp_spend_dist.columns:\n",
    "#             print(f\"\\n{metric}:\")\n",
    "#             print(\"Python:\", py_model.all_decomp_spend_dist[metric].describe())\n",
    "#             print(\"R:\", r_model.all_decomp_spend_dist[metric].describe())\n",
    "\n",
    "\n",
    "# def compare_media_effect_calculations(py_model, r_model):\n",
    "#     \"\"\"Compare media effect calculations between Python and R\"\"\"\n",
    "#     # Compare unique media channels\n",
    "#     py_media = py_model.all_decomp_spend_dist[\"rn\"].unique()\n",
    "#     r_media = r_model.all_decomp_spend_dist[\"rn\"].unique()\n",
    "\n",
    "#     print(\"\\nMedia channels comparison:\")\n",
    "#     print(\"Python channels:\", sorted(py_media))\n",
    "#     print(\"R channels:\", sorted(r_media))\n",
    "\n",
    "#     # Compare aggregated effects per channel\n",
    "#     print(\"\\nAggregated effects per channel:\")\n",
    "#     py_effects = py_model.all_decomp_spend_dist.groupby(\"rn\")[\"xDecompAgg\"].mean()\n",
    "#     r_effects = r_model.all_decomp_spend_dist.groupby(\"rn\")[\"xDecompAgg\"].mean()\n",
    "\n",
    "#     print(\"\\nPython effects:\")\n",
    "#     print(py_effects)\n",
    "#     print(\"\\nR effects:\")\n",
    "#     print(r_effects)\n",
    "\n",
    "\n",
    "# # Run comparisons\n",
    "# print(\"Running detailed comparisons between Python and R outputs...\")\n",
    "# compare_x_decomp_agg(output_models, r_output_models)\n",
    "# compare_decomp_spend_dist(output_models, r_output_models)\n",
    "# compare_media_effect_calculations(output_models, r_output_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_structure(data):\n",
    "    print(\"Columns:\", data.columns.tolist())\n",
    "    print(\"\\nFirst row:\", data.iloc[0].to_dict())\n",
    "    print(\"\\nShape:\", data.shape)\n",
    "\n",
    "\n",
    "# Assuming you want to print the structure for the first trial\n",
    "first_trial_r = r_output_models.trials[0].decomp_spend_dist\n",
    "first_trial_python = output_models.trials[0].decomp_spend_dist\n",
    "\n",
    "print(\"R exported data structure:\")\n",
    "print_data_structure(first_trial_r)\n",
    "\n",
    "# With Python calculated data\n",
    "print(\"\\nPython calculated data structure:\")\n",
    "print_data_structure(first_trial_python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytestenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
