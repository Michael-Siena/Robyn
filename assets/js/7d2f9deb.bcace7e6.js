"use strict";(self.webpackChunkmmm_for_all=self.webpackChunkmmm_for_all||[]).push([[672],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return u}});var i=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,i,n=function(e,t){if(null==e)return{};var a,i,n={},o=Object.keys(e);for(i=0;i<o.length;i++)a=o[i],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)a=o[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=i.createContext({}),d=function(e){var t=i.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=d(e.components);return i.createElement(l.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},h=i.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=d(a),h=n,u=m["".concat(l,".").concat(h)]||m[h]||c[h]||o;return a?i.createElement(u,r(r({ref:t},p),{},{components:a})):i.createElement(u,r({ref:t},p))}));function u(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,r=new Array(o);r[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:n,r[1]=s;for(var d=2;d<o;d++)r[d]=a[d];return i.createElement.apply(null,r)}return i.createElement.apply(null,a)}h.displayName="MDXCreateElement"},1770:function(e,t,a){a.r(t),a.d(t,{assets:function(){return m},contentTitle:function(){return d},default:function(){return f},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return c}});var i=a(7462),n=a(3366),o=(a(7294),a(3905)),r=a(4996),s=["components"],l={id:"features",title:"Key Features"},d=void 0,p={unversionedId:"features",id:"features",title:"Key Features",description:"An in-depth discussion of both the implementation and technical underpinnings of Robyn follows.",source:"@site/docs/features.mdx",sourceDirName:".",slug:"/features",permalink:"/Robyn/docs/features",draft:!1,editUrl:"https://github.com/facebookexperimental/Robyn/edit/main/website/docs/features.mdx",tags:[],version:"current",frontMatter:{id:"features",title:"Key Features"},sidebar:"someSidebar",previous:{title:"An Analyst's Guide to MMM",permalink:"/Robyn/docs/analysts-guide-to-MMM"},next:{title:"Robyn API for Python",permalink:"/Robyn/docs/robyn-api"}},m={},c=[{value:"Model Inputs",id:"model-inputs",level:2},{value:"Paid Media Variables",id:"paid-media-variables",level:3},{value:"Organic Variables",id:"organic-variables",level:3},{value:"Examples of typical organic variables",id:"examples-of-typical-organic-variables",level:4},{value:"Contextual Variables",id:"contextual-variables",level:3},{value:"The &quot;demand map&quot;",id:"the-demand-map",level:3},{value:"Variable Transformations",id:"variable-transformations",level:2},{value:"Adstock",id:"adstock",level:3},{value:"Geometric",id:"geometric",level:4},{value:"Weibull PDF &amp; CDF",id:"weibull-pdf--cdf",level:4},{value:"Saturation",id:"saturation",level:3},{value:"Trend &amp; seasonality decomposition with Prophet",id:"trend--seasonality-decomposition-with-prophet",level:2},{value:"Exemplary time-series decomposition",id:"exemplary-time-series-decomposition",level:3},{value:"Ridge Regression",id:"ridge-regression",level:2},{value:"Multi-Objective Hyperparameter Optimization with Nevergrad",id:"multi-objective-hyperparameter-optimization-with-nevergrad",level:2},{value:"Objective Functions",id:"objective-functions",level:3},{value:"Hyperparameters",id:"hyperparameters",level:3},{value:"Time Series Validation",id:"time-series-validation",level:2},{value:"Model Clustering",id:"model-clustering",level:2},{value:"Calibration with causal experiments",id:"calibration-with-causal-experiments",level:2},{value:"Key findings of calibration",id:"key-findings-of-calibration",level:3},{value:"Types of experiments",id:"types-of-experiments",level:3},{value:"Model onepager",id:"model-onepager",level:2},{value:"Budget allocation",id:"budget-allocation",level:2},{value:"Model refresh",id:"model-refresh",level:2}],h={toc:c},u="wrapper";function f(e){var t=e.components,a=(0,n.Z)(e,s);return(0,o.kt)(u,(0,i.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"An in-depth discussion of both the implementation and technical underpinnings of Robyn follows."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"model-inputs"},"Model Inputs"),(0,o.kt)("p",null,"The function ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_inputs()")," mainly captures all model specification for the dataset. Here we break down some of the underlying concepts"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'InputCollect <- robyn_inputs(\n  dt_input = dt_simulated_weekly\n  ,dt_holidays = dt_prophet_holidays\n  ,date_var = "DATE" # date format must be "2020-01-01"\n  ,dep_var = "revenue" # there should be only one dependent variable\n  ,dep_var_type = "revenue" # "revenue" or "conversion"\n  ,prophet_vars = c("trend", "season", "holiday") # "trend","season", "weekday" & "holiday"\n  ,prophet_country = "DE"# select only one country\n  ,context_vars = c("competitor_sales_B", "events") # e.g. competitors, discount, unemployment etc\n  ,paid_media_spends = c("tv_S","ooh_S","print_S","facebook_S", "search_S") # Media spend\n  ,paid_media_vars = c("tv_S", "ooh_S","print_S","facebook_I","search_clicks_P") # Media exposure metrics. Use the same as paid_media_spends if not available\n  ,organic_vars = c("newsletter") # marketing activity without media spend\n  ,factor_vars = c("events") # specify which variables in context_vars or organic_vars are factorial\n  ,window_start = "2016-01-01"\n  ,window_end = "2018-12-31"\n  ,adstock = "geometric" # geometric, weibull_cdf or weibull_pdf.\n)\n')),(0,o.kt)("h3",{id:"paid-media-variables"},"Paid Media Variables"),(0,o.kt)("p",null,"Refer to the ",(0,o.kt)("a",{parentName:"p",href:"https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM#data-collection"},"Data Collection")," section of the Analyst's Guide to MMM for a detailed discussion on best practices for selecting your paid media variables."),(0,o.kt)("p",null,"For paid media variables, we currently require that users designate both ",(0,o.kt)("inlineCode",{parentName:"p"},"paid_media_vars")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"paid_media_spends"),". ",(0,o.kt)("inlineCode",{parentName:"p"},"paid_media_vars")," refers to media exposure metrics, e.g. TV GRP, search clicks or Facebook impressions. ",(0,o.kt)("inlineCode",{parentName:"p"},"paid_media_spends")," refers to media spending. The two vectors must have the same length and the same order of media. When exposure metrics are not available, use the same as in ",(0,o.kt)("inlineCode",{parentName:"p"},"paid_media_spends"),"."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note"),": Robyn currently fits the main model using only media spend. Exposure as modeling varibale is deprecated due to the uncertainty it introduces to the budget allocation. Robyn uses the Michaelis-Menten function to translate between spend and exposure. Despite the nonlinear flexibility this function provides, the relationship between spend and exposure is often too complex for a univariate transformation. In earlier versions, this inaccuracy in spend-exposure translation has caused unreliable budget allocation. Even though we understand the narrative of using exposure metrics, we've decided to trade it off against the reliability of budget allocation. We'll re-introduce exposure fitting as soon as we have a working solution."),(0,o.kt)("p",null,"Despite the deprecation mentioned above, there's still value in providing exposure metrics in Robyn. If there's weak association detected between spend & exposure, it indicates that exposure metrics have different underlying pattern than spends. In this case, Robyn will recommend to splitting the channel for potentially better modeling result. Take Meta as example, retargeting and prospecting campaigns might have very different CPMs and efficiencies. In which case, it would be meaningful to split Meta by retargeting and prospecting."),(0,o.kt)("p",null,"In general, it's important to ensure that the paid media data is complete and accurate before proceeding. We will talk more about the variable transformations paid media variables will be subject to in the Variable Transformation section."),(0,o.kt)("h3",{id:"organic-variables"},"Organic Variables"),(0,o.kt)("p",null,"Robyn enables users to specify ",(0,o.kt)("inlineCode",{parentName:"p"},"organic_vars")," to model marketing activities without direct spend. Typically, this may include newsletter, push notification, social media post stats, among others. Moreover, organic variables are expected to have carryover (adstock) and saturating behavior as paid media variables. The respective transformation techniques are also applied to organic variables. More on these transformations in the following section."),(0,o.kt)("h4",{id:"examples-of-typical-organic-variables"},"Examples of typical organic variables"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Reach / impressions on blog posts"),(0,o.kt)("li",{parentName:"ul"},"Impressions on organic & unpaid social media"),(0,o.kt)("li",{parentName:"ul"},"SEO improvements"),(0,o.kt)("li",{parentName:"ul"},"Email campaigns"),(0,o.kt)("li",{parentName:"ul"},"Reach on UGC")),(0,o.kt)("h3",{id:"contextual-variables"},"Contextual Variables"),(0,o.kt)("p",null,"All contextual variables must be specified as elements in ",(0,o.kt)("inlineCode",{parentName:"p"},"context_vars"),". For a detailed discussion on potential contextual variables to include in your model, see the ",(0,o.kt)("a",{parentName:"p",href:"https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM#data-collection"},"Data Collection")," section of the Analyst's guide to MMM."),(0,o.kt)("h3",{id:"the-demand-map"},'The "demand map"'),(0,o.kt)("p",null,"A demand map is a very helpful practise to identify all relevant factors influencing our business performance. It's strongly recommended to do this exercise, esp. when running MMM for the first time. It's also helpful to revisit your hypothesis regularly."),(0,o.kt)("img",{alt:"organic media",src:(0,r.Z)("/img/organic-media.png")}),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"variable-transformations"},"Variable Transformations"),(0,o.kt)("p",null,"MMM is typlically characterised by the following two hypothesis:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Ads investment has lagged effect / carries over through time. For example, I see ads today and buy next week."),(0,o.kt)("li",{parentName:"ol"},"Ads investment has diminishing returns. For example, the more I spent on a channel, the less marginal return I will get.")),(0,o.kt)("p",null,"Robyn conducts two types of transformation to account for these hypothesis:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Adstock transformation"),(0,o.kt)("li",{parentName:"ol"},"Saturation transformation")),(0,o.kt)("h3",{id:"adstock"},"Adstock"),(0,o.kt)("p",null,'Adstock refers to the hypothesis of ads carryover and reflects the theory that the effects of advertising can lag and\ndecay following initial exposure. It\'s also related to certain brand equity metrics like ad-recall or campaign awareness. The logical narrative is "I saw the ads X days ago before I bought the product". Usually, it\'s assumed that this "memory" decays as time passes. But there\'re also cases when it\'s legitimate to assume that the effect of this "memory" will increase first before decreasing. For example, expensive products like cars or credits are unlikely to be purchased directly after the ads, esp. for offline channels. Therefore, it\'s usual to assume lagged effect with later peaks for offline channels for these products. At the same time, online conversions from digital channels might be suitable without lagged peaks and only deal with decay.'),(0,o.kt)("p",null,"The formula for adstock can be generalised as:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"media_adstocked_i = media_raw_i + decay_rate *  media_raw_i-1"),", where ",(0,o.kt)("inlineCode",{parentName:"p"},"i")," is a given time period."),(0,o.kt)("p",null,"There are three adstock transformation options you can choose in Robyn:"),(0,o.kt)("h4",{id:"geometric"},"Geometric"),(0,o.kt)("p",null,"The one-parametric exponential decay function is used with theta as the fixed-rate decay parameter. For example, an adstock of ",(0,o.kt)("inlineCode",{parentName:"p"},"theta = 0.75")," means that 75% of the ads in Period 1 carryover to Period 2. Robyn's implementation of Geometric transformation can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/R/R/transformation.R#L57"},"here")," and is shown conceptually as followed:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"media_adstocked_ij = media_raw_ij + decay_rate_j *  media_raw_i-1_j"),", where ",(0,o.kt)("inlineCode",{parentName:"p"},"i")," is a given time period and ",(0,o.kt)("inlineCode",{parentName:"p"},"j")," depicts a media variable. The ",(0,o.kt)("inlineCode",{parentName:"p"},"decay_rate_j")," is a constant value per media variable and equal to the Geometric parameter ",(0,o.kt)("inlineCode",{parentName:"p"},"theta"),"."),(0,o.kt)("p",null,"Some rule of thumb estimates we have found from historically building weekly-level models are that  TV has tended to have adstock between 0.3 - 0.8, OOH/Print/Radio has had 0.1-0.4, and Digital has had 0.0 - 0.3. This is anecdotal advice so please use your best judgement when building your own models."),(0,o.kt)("img",{alt:"Example Geometric Adstocks",src:(0,r.Z)("img/adstockintro.png")}),(0,o.kt)("p",null,"Another useful property of Geometric decay is that the limit of the inifinite sum is equal to ",(0,o.kt)("inlineCode",{parentName:"p"},"1 / (1 - theta)"),". For example, when ",(0,o.kt)("inlineCode",{parentName:"p"},"theta = 0.75"),", its infinite sum is ",(0,o.kt)("inlineCode",{parentName:"p"},"1 / (1 - 0.75) = 4"),'. Because Robyn conducts adstock transformation on spends, it can give you a quick and inituitive idea of how much "inflation" the adstock transformation will add on your raw data. On the other hand, Geometric decay is not able to capture more flexible time-varying decay, nor the lagging effect, which are solved by the Weibull adstock below.'),(0,o.kt)("h4",{id:"weibull-pdf--cdf"},"Weibull PDF & CDF"),(0,o.kt)("p",null,"Robyn offers the two-parametric Weibull function in the formats of PDF and CDF. Compared to the one-parametric Geometric function where theta is equal to the fixed decay rate, Weibull produces a vector of time-varying decay rates through more flexibility in the transformation with the parameters ",(0,o.kt)("inlineCode",{parentName:"p"},"shape")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"scale"),". Robyn's implementation of Weibull transformation can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/R/R/transformation.R#L128"},"here")," and is shown conceptually as followed:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"media_adstocked_ij = media_raw_ij + decay_rate_ij *  media_raw_i-1_j"),", where ",(0,o.kt)("inlineCode",{parentName:"p"},"i")," is a given time period and ",(0,o.kt)("inlineCode",{parentName:"p"},"j")," depicts a media variable. Note that the ",(0,o.kt)("inlineCode",{parentName:"p"},"decay_rate_ij")," specific to time period ",(0,o.kt)("inlineCode",{parentName:"p"},"i")," of the media variable ",(0,o.kt)("inlineCode",{parentName:"p"},"j"),", compared to the fixed decay rate from Geometric."),(0,o.kt)("p",null,"The plot below shows the flexibility of Weibull adstocks with regard to the two parameters ",(0,o.kt)("inlineCode",{parentName:"p"},"shape")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"scale"),". This flexibility does come at a cost of more computational power due to the extra hyperparameters. ",(0,o.kt)("strong",{parentName:"p"},"Weibull PDF is strongly recommmended for products with longer conversion window.")," We've seen Weibulll PDF leading to considerabaly better fit in some cases."),(0,o.kt)("img",{alt:"Example Weibull Adstocks",width:"800",src:(0,r.Z)("img/weibulladstocks.png")}),(0,o.kt)("p",null," ",(0,o.kt)("strong",{parentName:"p"},"Weibull CDF adstock:")," The Cumulative Distribution Function of Weibull has two parmeters\n",(0,o.kt)("inlineCode",{parentName:"p"},"shape")," & ",(0,o.kt)("inlineCode",{parentName:"p"},"scale")," that enables flexible decay rate, compared to Geometric adstock with fixed\ndecay rate. The ",(0,o.kt)("inlineCode",{parentName:"p"},"shape")," parameter controls the curvature of the decay curve. Recommended\nbound is ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.0001, 2)"),". The larger the shape, the more S-form the curve becomes. The smaller\nthe shape, the more L-form. ",(0,o.kt)("inlineCode",{parentName:"p"},"scale")," controls the inflexion point of the curve. We recommend\nvery conservative bounce of ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0, 0.1)"),", because scale increases the adstock half-life greatly."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Weibull PDF adstock:")," The Probability Density Function of the Weibull also has two\nparameters, ",(0,o.kt)("inlineCode",{parentName:"p"},"shape")," & ",(0,o.kt)("inlineCode",{parentName:"p"},"scale"),", and also provides flexible decay rate as Weibull CDF. The\ndifference is that Weibull PDF offers lagged effect. When ",(0,o.kt)("inlineCode",{parentName:"p"},"shape > 2"),", the curve peaks\nafter x = 0 and has NULL slope at x = 0, enabling lagged effect and sharper increase and\ndecrease of adstock, while the scale parameter indicates the limit of the relative\nposition of the peak at x axis; when ",(0,o.kt)("inlineCode",{parentName:"p"},"1 < shape < 2"),", the curve peaks after x = 0 and has\ninfinite positive slope at x = 0, enabling lagged effect and slower increase and decrease\nof adstock, while scale has the same effect as above; when ",(0,o.kt)("inlineCode",{parentName:"p"},"shape = 1"),", the curve peaks at\nx = 0 and reduces to exponential decay, while scale controls the inflexion point; when\n",(0,o.kt)("inlineCode",{parentName:"p"},"0 < shape < 1"),", the curve peaks at x = 0 and has increasing decay, while scale controls\nthe inflexion point. When all possible shapes are relevant, we recommend ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.0001, 10)"),"\nas bounds for ",(0,o.kt)("inlineCode",{parentName:"p"},"shape"),"; when only strong lagged effect is of interest, we recommend\n",(0,o.kt)("inlineCode",{parentName:"p"},"c(2.0001, 10)")," as bound for ",(0,o.kt)("inlineCode",{parentName:"p"},"shape"),". In all cases, we recommend conservative bound of\n",(0,o.kt)("inlineCode",{parentName:"p"},"c(0, 0.1)")," for ",(0,o.kt)("inlineCode",{parentName:"p"},"scale"),". Due to the great flexibility of Weibull PDF, meaning more freedom\nin hyperparameter spaces for Nevergrad to explore, it also requires larger iterations\nto converge."),(0,o.kt)("h3",{id:"saturation"},"Saturation"),(0,o.kt)("p",null,"The theory of diminishing returns (also called saturation) holds that each additional\nunit of advertising investment increases the response at a declining rate. This\nhypothesis is implemented as variable transformation in marketing mix models."),(0,o.kt)("img",{alt:"Diminishing returns1",src:(0,r.Z)("img/diminishingreturns1.png")}),(0,o.kt)("p",null,"The nonlinear response to a media variable on the dependent variable can be\nmodelled in different ways. Common approaches include transforming the media variable\nby the logarithm transformation, the power transformation or any sigmoid-shape\nfunctions. A common trait of these transformation is that they are all continuous\nand differentiable, which is important for the budget allocation with any nonlinear\noptimisation algorithms."),(0,o.kt)("p",null,"Robyn uses the latter and implements the Hill function for a flexible\ntransformation into S- or L-shape saturation. The implementation of Hill function\nin Robyn can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/R/R/transformation.R#L210"},"here"),"\nand is shown conceptually as followed:"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"media_saturated_j = 1 / (1 + (gamma_j / media_adstocked_j) ^ alpha_j)"),", where\n",(0,o.kt)("inlineCode",{parentName:"p"},"j")," depicts a media variable."),(0,o.kt)("p",null,"Note that ",(0,o.kt)("inlineCode",{parentName:"p"},"gamma_j")," above is scaled to the inflexion point of the variable. By\nvarying the values of ",(0,o.kt)("inlineCode",{parentName:"p"},"alpha")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"gamma"),", changes in curvature and inflexion point\nof the saturation curve can be observed below:"),(0,o.kt)("img",{alt:"Diminishing returns1",src:(0,r.Z)("img/diminishingreturns3.png")}),(0,o.kt)("p",null,"In this plot, x axis depicts spend and y response. ",(0,o.kt)("inlineCode",{parentName:"p"},"alpha")," controls the shape of\nthe curve between C- and S-shape. Recommended bound for ",(0,o.kt)("inlineCode",{parentName:"p"},"alpha")," is ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.5, 3)"),".\nThe larger the alpha, the more S-shape. The smaller, the more C-shape. ",(0,o.kt)("inlineCode",{parentName:"p"},"gamma"),"\ncontrols the inflexion point of the curve. Recommended bounce of ",(0,o.kt)("inlineCode",{parentName:"p"},"gamma")," is\n",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.3, 1)"),". The larger the gamma, the later the inflection point in the response\ncurve."),(0,o.kt)("p",null,'Another important term that\'s related to saturation is the marginal response. In\nmathematical term, the marginal response is equal to the first derivative of a given\npoint at the nonlinear curve. In layman\'s term, it\'s the "next dollar response" on\na given spend level. For example, if the average weekly spend is 1000$, "what is the\nreturn from the 1001th dollar?" This is the foundation of understanding how nonlinear\nbudget allocation works. For more details in marginal response, see ',(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("a",{parentName:"strong",href:"https://medium.com/@gufengzhou/the-convergence-of-marginal-roas-in-the-budget-allocation-in-robyn-5d407aebf021"},"this article"),': "The convergence\nof marginal ROAS in the budget allocation in Robyn"'),"."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"trend--seasonality-decomposition-with-prophet"},"Trend & seasonality decomposition with Prophet"),(0,o.kt)("p",null,"As for any real-life time-series problems, it's fundamental to understand trend\nand seasonality in the data for better fitting. In conventional MMM, this is often\ndone separately and manually using simple assumptions like linear trend or dummy\nvariable for seasonality. With the purpose of reducing human bias in the modeling\nprocess, Robyn provides trend, season, holiday & weekday decomposition out of the\nbox using Prophet, a popular time-series forecast package by Meta. In many cases,\nthis has proven to be increasing the model fit greatly. More details on how time-series\ndecomposition works can be found in ",(0,o.kt)("a",{parentName:"p",href:"https://facebook.github.io/prophet/docs/trend_changepoints.html"},"Prophet's documentaion"),"."),(0,o.kt)("h3",{id:"exemplary-time-series-decomposition"},"Exemplary time-series decomposition"),(0,o.kt)("p",null,'In the plot below, the dependent variable "revenue" from Robyn\'s weekly ',(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/tree/main/R/data"},"sample data"),' is decomposed into trend, season and holiday components. Note that\nweekday is not included because it\'s only available for daily data. Note that the "event"\nvariable is included as an extra regressor. With this, Robyn "converts" a categorical\nvariable to a continuous one, which simplifies later calculation.'),(0,o.kt)("img",{alt:"prophet 2",src:(0,r.Z)("/img/prophet_decomp.png")}),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"ridge-regression"},"Ridge Regression"),(0,o.kt)("p",null,"Marketing Mix Models often encounter the multicollinear issue, because it is a\ncommon business practise to operate on different media channels at the same time\n(e.g. increasing spends on all channels over Christmas). In other words, multicollinearity\noccurs when more than one independent variables are correlated in a regression\nmodel. It results in unstable coefficient estimates, difficulty in interpretation\nof the coefficient significance as well as overfitting."),(0,o.kt)("p",null,"Regularised regression is a common choice to counter the challenges of multicollinearity\nmentioned above. Robyn uses the L2-form of the regularisation, also known as Ridge\nregression. We've decided for Ridge instead of Lasso (L1-form) regression, because\nLasso tends to eliminate features (\"zero-out\") while Ridge rather reduces their\nmagnitude. For MMM, it's unrealistic to have many media variables having zero effect."),(0,o.kt)("p",null,"Robyn uses the package ",(0,o.kt)("inlineCode",{parentName:"p"},"glmnet")," to perform the regularised regression fitting.\nThe implementation can be found ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/R/R/model.R#L1047"},"here"),".\nThe loss function for Ridge regression is as followed:"),(0,o.kt)("img",{alt:"Ridge Regression Formula",src:(0,r.Z)("img/Ridge.png")}),(0,o.kt)("p",null,"The additive model equation is as followed:"),(0,o.kt)("img",{alt:"Ridge Regression Formula",src:(0,r.Z)("img/model_specification.png")}),(0,o.kt)("p",null,'A popular question about our model choice is "why not use a Bayesian model". The Bayesian\nframework has seen rising popularity for business implementations, especially because\nof many of its attractive properties like the Bayesian prior as a native model calibration\nfeature and the intuitive interpretation of the Bayesian credible interval.'),(0,o.kt)("p",null,"We want to point out that Ridge regression is equivilent to a Bayesian regression with a\nnormal / Gaussian prior, while Lasso corresponds to the Bayesian Laplace prior. This\nis a well-studied fact academically, for example in the paragraph ",(0,o.kt)("strong",{parentName:"p"},'"Bayesian Interpretation of\nRidge Regression and the Lasso"')," on page 249 in ",(0,o.kt)("a",{parentName:"p",href:"https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf"},'"An Introduction\nto Statistical Learning"'),"\nby T.Hastie and co."),(0,o.kt)("p",null,"Robyn uses an multi-objective algorithm as the optimisation approach,because we\nbelieve that the problem a MMM tries to solve has multiple goals. It's often said\nthat MMM is both science and craft. The model is expected to both predict well and\n\"makes sense\". From this angle, it's very natural to consider this a multi-objective\noptimisation problem. Regularised regression has a better fit to our optimizer choice\n(TwoPointsDE, an evolutionary algorithm from Nevergrad), because the (hyper)parameters\nare estimated with many evolutionary iterations, as opposed to the typical sampling\nprocess with MCMC in a Bayesian regression. More details about multi-objective\noptimisation and Nevergrad see below."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"multi-objective-hyperparameter-optimization-with-nevergrad"},"Multi-Objective Hyperparameter Optimization with Nevergrad"),(0,o.kt)("p",null,'Every MMM practitioner knows this "sanity check": When facing multiple model candidates, it\'s common to prefer a model that\'s more "plausible", meaning one with slightly less fit but better alignment with current spend allocation. For example, for a dataset with 2 channels splitting 90%/10% media spend, a model that produces the contribution of 10%/90% is considered rather implausible, no matter how good the model fit is. In comparison, another model with 70%/30% contribution split would be considered more pausible.'),(0,o.kt)("p",null,'In other words, a good MMM needs to be both predictive and interpretable. This is the frequently cited conflict of science and craft. At team Robyn, we considered this a multi-objective optimisation problem, because the reality is never simply prediction. We believe it\'s important to include the "sanity check" into the optimisation through parameterization in the form of extra objective functions. ',(0,o.kt)("strong",{parentName:"p"},"The implementation of multi-objective hyperparameter optimization is considered the most important innovation in Robyn.")," At the same time, the usage of hyperparameters enables stronger automation of the parameter selection for adstocking, saturation, regularization penalty and even training size of time-series validation."),(0,o.kt)("p",null,"Robyn uses ",(0,o.kt)("a",{parentName:"p",href:"https://facebookresearch.github.io/nevergrad/"},(0,o.kt)("strong",{parentName:"a"},"Nevergrad")),', Meta\u2019s gradient-free optimization platform to perform this task with its so-called "ask & tell"" interface. Simply explained, Robyn "asks" Nevergrad for the mutating hyperparameter values by "telling" it which values have better scores (objective functions).'),(0,o.kt)("h3",{id:"objective-functions"},"Objective Functions"),(0,o.kt)("p",null,'Robyn implements three objective functions as the "goals" for hyperparameter optimisation currently.'),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"NRMSE"),": The Normalized Root Mean Square Error is also referred to as the ",(0,o.kt)("strong",{parentName:"li"},"prediction error"),". Robyn allows time-series validation with the spitting of the dataset into train / validation / test. When fitting without the time-series validation, the training error ",(0,o.kt)("inlineCode",{parentName:"li"},"nrmse_train")," is objective function for the evolving iterations. With time-series validation, the validation error ",(0,o.kt)("inlineCode",{parentName:"li"},"nrmse_val")," becomes the objective function, while ",(0,o.kt)("inlineCode",{parentName:"li"},"nrmse_test")," is used to assess the out-of-sample preditive performance."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"DECOMP.RSSD"),": The Decomposition Root Sum of Squared Distance is also referred to as the ",(0,o.kt)("strong",{parentName:"li"},"business error"),' and is a key invention of Robyn. It represents the difference between share of spend and share of effect for paid media variables. We\'re aware that this metric is controvertial because of the convergence of media ROAS. In the reality, multiple objectives always "work together" and trade off each other in the optimisation process. DECOMP.RSSD rules out models with extreme decomposition and helps narrowing down model selection.'),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"MAPE.LIFT"),": The Mean Absolute Percentage Error for experiments is activated when calibrating and is referred to as the ",(0,o.kt)("strong",{parentName:"li"},"calibration error"),". It's a key invention of Robyn and allows Robyn to minimise the difference between predicted effect and causal effect.")),(0,o.kt)("h3",{id:"hyperparameters"},"Hyperparameters"),(0,o.kt)("p",null,"There're four types of hyperparameters in Robyn at the moment."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Adstocking"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"theta")," when selecting Geometric adstocking, or ",(0,o.kt)("inlineCode",{parentName:"li"},"shape")," & ",(0,o.kt)("inlineCode",{parentName:"li"},"scale")," when selecting Weibull adstocking"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Saturation"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"alpha")," & ",(0,o.kt)("inlineCode",{parentName:"li"},"gamma")," for Hill function"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Regularization"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"lambda")," for the penalty term in ridge regression"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Validation"),": ",(0,o.kt)("inlineCode",{parentName:"li"},"train_size")," for the percentage of training data")),(0,o.kt)("p",null,"The cardinality of hyperparameters increases when adding more paid and organic media variables, because Robyn performs adstocking & saturation transformation for each media variable individually. For example, if using 10 media variables with Geometric adstock, the total amount of hyperparameters is 32: 10 thetas + 10 alphas + 10 gammas + 1 lambda + 1 train_size. With Weibull adstock, it's 42: 10 shapes + 10 scales + 10 alphas + 10 gammas + 1 lambda + 1 train_size. Adding hyperparameters will give Robyn more flexibility to find optimum solutions, but it will trade-off model runtime because it needs longer to converge."),(0,o.kt)("p",null,"Using the concept of ",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Pareto_efficiency"},(0,o.kt)("strong",{parentName:"a"},"Pareto-optimality")),' balancing all objective functions, Robyn always outputs a set of pareto-optimum model candidates that are considered "the best". Please find below an example of an example chart of the Pareto model solutions.\nEach dot in the chart represents an explored model solution, while the lower-left corner lines are Pareto-fronts 1-3 and contains the best possible model results from all iterations.\nThe two axes (NRMSE on x and DECOMP.RSSD on y) are the two objective functions to be minimized.\nAs the iteration increases, a trend down the lower-left corner of the coordinate can be clearly observed.\nThis is a proof of Nevergrad\'s ability to drive the model result towards an optimal direction.'),(0,o.kt)("img",{alt:"pareto chart",src:(0,r.Z)("/img/pareto_front.png")}),(0,o.kt)("p",null,"The premise of an ",(0,o.kt)("strong",{parentName:"p"},"evolutionary algorithm")," is that of natural selection. In an evolutionary algorithm you may have a set of iterations where some combinations of coefficients that will be explored by the model will survive and proliferate, while unfit models will die off and not contribute to the gene pool of further generations, much like in natural selection.\nIn robyn, we recommend a minimum of 2000 iterations where each of these will provide feedback to its upcoming generation, and therefore guide the model towards the optimal coefficient values for alphas, gammas and thetas. We also recommend a minimum of 5 trials which are a set of independent initiations of the model that will each of them have the number of iterations you set under \u2018set_iter\u2019 object. E.g. 2000 iterations on set_iter x 5 trials = 10000 different iterations and possible model solutions."),(0,o.kt)("p",null,"In Robyn, we consider the model to be converged ",(0,o.kt)("strong",{parentName:"p"}," UNDER REVISION ")," when:"),(0,o.kt)("p",null,"Criteria #1:\nLast quantile's standard deviation < first 3 quantiles' mean standard deviation"),(0,o.kt)("p",null,"Criteria #2:\nLast quantile's absolute median < absolute first quantile's absolute median - 2 * first 3 quantiles' mean standard deviation"),(0,o.kt)("p",null,"The quantiles are ordered by the iterations of the model, so if we ran 1000 iterations, the first 200 iterations would make up the first quantile. These two criteria represent an effort to demonstrate that both the standard deviation and the mean for both NRMSE and DECOMP.RSSD have improved relative to where they began, and they are not as variable."),(0,o.kt)("p",null,"Alternatively, run the following code to observe the convergence of your multi-objective optimization in the ridgeline visualization."),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"OutputModels$convergence$moo_distrb_plot")),(0,o.kt)("img",{alt:"moo cloud plot",src:(0,r.Z)("img/moo_cloud_plot.png")}),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"time-series-validation"},"Time Series Validation"),(0,o.kt)("p",null,"When ",(0,o.kt)("inlineCode",{parentName:"p"},"ts_validation = TRUE")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_run()")," a 3-way-split time series for NRMSE validation is enabled.\nA time series validation parameter ",(0,o.kt)("inlineCode",{parentName:"p"},"train_size")," is included as one of Robyn's hyperparameters. When ",(0,o.kt)("inlineCode",{parentName:"p"},"ts_validation = TRUE")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_run()"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"train_size")," defines the percentage of data used for training, validation and out-of-sample testing. For example, when ",(0,o.kt)("inlineCode",{parentName:"p"},"train_size = 0.7"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"val_size")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"test_size")," will be 0.15 each. This hyperparameter is customizable or can be fixed with a default range of ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.5, 0.8)")," and must be between ",(0,o.kt)("inlineCode",{parentName:"p"},"c(0.1, 1)"),"."),(0,o.kt)("img",{alt:"time series validation",src:(0,r.Z)("img/time_series_validation_and_convergence.png")}),(0,o.kt)("img",{alt:"time series validation",src:(0,r.Z)("img/actual_vs_predicted_response_ts.png")}),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"model-clustering"},"Model Clustering"),(0,o.kt)("p",null,"As depicted in plot 4 in ",(0,o.kt)("a",{parentName:"p",href:"#model-onepager"},"session model onepager")," below, the k-means clustering is used to further reduce model choice. Robyn uses bootstrapping to calculate uncertainty of ROAS or CPA."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Use k-means clustering on all pareto-optimal model candidates to find clusters of models with similar hyperparameters. The constant k describes the number of clusters."),(0,o.kt)("li",{parentName:"ul"},"When ",(0,o.kt)("inlineCode",{parentName:"li"},'k = "auto"')," (default value), WSS (Within Group Sum of Squares) is calculated to automatically find the best k value between 1-20. The k value with <5% WSS change from the previous k is selected. This is visualised in the screenshot below."),(0,o.kt)("li",{parentName:"ul"},"Within each of the k clusters, a winner model is selected based on the three normalised objective functions (NRMSE, DECOM.RSSD, and MAPE if calibrated was used)."),(0,o.kt)("li",{parentName:"ul"},"In the context of ",(0,o.kt)("a",{parentName:"li",href:"#multi-objective-hyperparameter-optimization-with-nevergrad"},"MOO"),", we consider each cluster a sub-population in a different local optimum.")),(0,o.kt)("img",{alt:"Pareto clusters WSS",src:(0,r.Z)("img/pareto_clusters_wss.png")}),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"calibration-with-causal-experiments"},"Calibration with causal experiments"),(0,o.kt)("p",null,"Randomised controlled trial (RCT) is an established academic gold standard to infer causality in science. By applying results from RCT in ads measurement that are considered ground truth, you can introduce causality into your marketing mix models. Robyn implements the MMM calibration as an objective function in the multi-objective optimization by parameterizing the difference between causal results and predicted media contribution."),(0,o.kt)("img",{alt:"Calibration chart",src:(0,r.Z)("/img/calibration1.png")}),(0,o.kt)("p",null,"The chart above illustrates the calibration concept. The calibration error ",(0,o.kt)("strong",{parentName:"p"},"MAPE.LIFT")," is implemented as the third objective function besides the prediction error and the business error (please refer to ",(0,o.kt)("a",{parentName:"p",href:"#multi-objective-hyperparameter-optimization-with-nevergrad"},"details above"),") and depicts the difference between predicted media contribution and the ground truth, as shown in the equation in the chart."),(0,o.kt)("h3",{id:"key-findings-of-calibration"},"Key findings of calibration"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"According to an third party whitepaper ",(0,o.kt)("a",{parentName:"li",href:"https://analytic-edge.com/blog/the-value-of-calibrating-mmm-with-lift-experiments/"},'"The Value of Calibrating MMM with Lift Experiments"')," by Analytic Edge, ",(0,o.kt)("strong",{parentName:"li"},"uncalibrated models show 25% average difference to the ground truth"),"."),(0,o.kt)("li",{parentName:"ul"},"Another ",(0,o.kt)("a",{parentName:"li",href:"https://medium.com/@gufengzhou/calibrated-mmm-better-predicts-true-roas-d5adfc8abdc4"},"simulated experiment")," conducted by the the winners of the ",(0,o.kt)("a",{parentName:"li",href:"https://apac-robyn2022.devpost.com/"},"Robyn 2022 Hackathon")," has found out that:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Calibrated models are better"),": All calibrated models show predicted ROAS closer to true ROAS with smaller MAPE than uncalibrated models."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Good for one, good for all"),": When only calibrating one from both simulated channels (TV & FB), the other channel also predicts better towards true ROAS than uncalibrated."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"The more calibrated channels, the better"),": Calibrating both simulated channels (TV & FB) shows the lowest error to true ROAS. This is expected, because Robyn\u2019s calibration is designed to recover the true ROAS by having the calibration error (MAPE.LIFT) as an objective function within Robyn\u2019s multi-objective optimisation capacity."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"The more studies to calibrate, the better"),": The true ROAS prediction improves strongly with up-to 10 studies per channel.")))),(0,o.kt)("h3",{id:"types-of-experiments"},"Types of experiments"),(0,o.kt)("p",null,"There're two major types of experiements in ads measurement, as pointed out by this WARC article ",(0,o.kt)("a",{parentName:"p",href:"https://www.warc.com/content/paywall/article/a-step-by-step-guide-to-calibrating-marketing-mix-models-with-experiments/en-GB/152212"},'"A step-by-step guide to calibrating marketing mix models with experiments"'),"."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Sales experiments"),": These experiments aim to measure the impact of marketing activities on sales such as online conversion. Examples are ",(0,o.kt)("a",{parentName:"li",href:"https://www.facebook.com/business/m/one-sheeters/conversion-lift"},"Meta\u2019s Conversion Lift")," or ",(0,o.kt)("a",{parentName:"li",href:"https://support.google.com/google-ads/answer/12003020?hl=en"},"Google\u2019s Conversion Lift"),". These experiments work by comparing the conversion rates between a test group and a control group, and help estimate the causal effect of marketing.",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Pros"),": Provide direct insight into sales impact, relatively straightforward to plan and execute."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Cons"),": Limited to measuring conversion-related outcomes, may overlook non-conversion effects. Only applicable to digital channels."))),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Geo experiments"),": Geo experiments analyze the impact of marketing activities in specific geographic regions such as DMAs, states or cities. By comparing the outcomes in regions exposed to marketing interventions with those in control regions, the spatial variation in response can be quantified.",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Pros"),": Capture spatial variations and market-specific responses, useful for localized marketing. Possibility to run offline. Same test for cross channel. Can estimate groups of channels (e.g. all digital). Might even be applicable for offline channels like addressable TV or OOH."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Cons"),": Could get affected by potential spillover effects across regions, requiring careful design to ensure comparability.")))),(0,o.kt)("p",null,"Robyn accepts a dataframe as calibration input in the ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_inputs()")," function. The function usage can be found in the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/demo/demo.R#L262"},"demo"),"."),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"model-onepager"},"Model onepager"),(0,o.kt)("p",null,"Robyn automatically exports various graphical and tabular outputs into the specified path to aid result interpretation and further custom analysis. An onepager per pareto-optimal model is exported after running the ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_outputs()")," function. An example:"),(0,o.kt)("img",{alt:"ModelResults1 chart",src:(0,r.Z)("/img/modelresults2.png")}),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Response decomposition waterfall:"),' This chart depicts the absolute as well as percentage contribution of all of independent variables. The percentages sum up to 100% and can be interpreted as "x% of the revenue can be explained by variable A". ',(0,o.kt)("strong",{parentName:"p"},'The concept "baseline"'),' might come up in the context of total decomposition. While there\'s no clear definition of baseline, we define baseline as "non-steerable variables". All marketing-related variables (paid, organic, price, promotion etc.) are considered steerable. The rest of seasonal (trend, season, holiday etc.), external variables (competitors, weather etc.) and intercept are considered baseline.')),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Actual vs. predicted response:"),' This chart shows the actual response variable (e.g. revenue) vs. the predicted value from the model. The goodness of fit can be observed visually. In this example, the R-squared is considerably high (close to 0.9), while both actual and predicted lines are visually well aligned. Moreover, this plot helps the interpretation of "unmatched events", e.g. the peak mid 2018 is not captured by the model at all. Was there a special event that\'s not accounted for in the model?')),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Share of spend vs. share of effect:")," This plot shows the comparison between the total effect share and total spend share for all paid media variables within the entire modeling window. This is the visualization of the objective function ",(0,o.kt)("a",{parentName:"p",href:"#objective-functions"},"DECOMP.RSSD"),". Moreover, the total return on investment (ROAS) of each media variable are also plotted. Note that the percentage of effect will sum up to 100% for media variable only in this plot, as opposed to the waterfall plot that contains all variabes.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"In-cluster bootstrapped confidence interval"),": Robyn uses bootstrapping to calculate uncertainty of ROAS or CPA. After obtaining all pareto-optimal model candidates, the K-means clustering is applied to find clusters of models with similar hyperparameters. In the context of ",(0,o.kt)("a",{parentName:"p",href:"#multi-objective-hyperparameter-optimization-with-nevergrad"},"MOO"),", we consider these clusters sub-population in different local optima. Then we bootstrap the efficiency metrics (ROAS or CPA) to obtain the distribution and the 95% interval.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Adstock decay rate:")," This chart represents the carryover effect of each media variable (paid & organic). For Geometric adstock, the barchart depicts the fixed decay rate. The higher the value, the stronger the carryover into the future. For Weibull adstock, each sub-chart depicts the time-varying decay rate for each media variable, which might also include lag effect. A common hypothesis of adstock is that offline channels have stronger the carryover.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Immediate vs. carryover"),': Robyn splits the media contribution into two parts: immediate contribution and carryover contribution. Immediate is the effect of the direct media spend of a given period, while carryover is the effect of historical spend "leaked" (also called adstock) into the given period. Conceptually, the carryover effect can be understood as the effect of upper-/mid-funnel brand equity metrics like "ad recall" or "campaign awareness". Note that we avoid using the terms "short-term" and "long-term", because we believe, without any agreed definition, that the "long-term effect" should describe media impact on baseline sales. In comparison, adstocking only addresses media\'s direct impact on the response and says nothing about the interaction with baseline. Note that it\'s possible to have a channel with higher carryover but lower adstock, as seen in this example above with ',(0,o.kt)("inlineCode",{parentName:"p"},"ooh_S")," vs ",(0,o.kt)("inlineCode",{parentName:"p"},"tv_S"),". The reason lies in the different saturation curves per media (explained below) that also have impact on the carryover decomposition.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Saturation curves:")," Saturation curve has many synonyms, like response curve or diminishing return. Robyn calculates one saturation curve (",(0,o.kt)("a",{parentName:"p",href:"#saturation"},"one set of Hill parameters"),") per media variable. The grey area depicts the historical carryover, as explained in point 6 above: The end of grey area projected on the Y-axis is the carryover effect, while the dot (average spend) projected on Y is the total effect. The difference between these two is the immediate effect. Use your business context to make sense of these curves. For example, an over-spending media variable would expect the dot on the flatter area of the curve, representing lower ",(0,o.kt)("strong",{parentName:"p"},"marginal ROAS")," or ",(0,o.kt)("strong",{parentName:"p"},"mROAS"),", the most important metric for budget allocation. For more details see this article ",(0,o.kt)("a",{parentName:"p",href:"https://medium.com/@gufengzhou/the-convergence-of-marginal-roas-in-the-budget-allocation-in-robyn-5d407aebf021"},'"The convergence of marginal ROAS in the budget allocation in Robyn"'),". Tip: You can also use the ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_response()")," function in to recreate these response curves. For details see the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/facebookexperimental/Robyn/blob/main/demo/demo.R#L518"},"demo"),".")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Fitted vs. residual:")," This chart shows the relationship between fitted and residual values. For a good fitting model, this plot is expected to show rather horizontal trend line with the dots evenly scattered around the X-axis. On the contrary, visible patterns (funnle shape, waves, groupped outliers) indicate missing variable in the model."))),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"budget-allocation"},"Budget allocation"),(0,o.kt)("p",null,"Robyn's budget allocator comes with two scenarios:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Maximum response"),': Given a total budget and media-level constraints, the allocator calculates the optimum cross-media budget split by maximising the response. For example: "What\'s the optimum media split if I have budget X?"'),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Target efficiency (ROAS or CPA)"),': Given a target ROAS or CPA and media-level constraints, the allocator calculates the optimum cross-media budget split and the total budget by maximising the response. For example: "What\'s the optimum media split and how much budget do I need if I have want to hit ROAS X?"')),(0,o.kt)("p",null,"Robyn uses the optimisation package \u201cnloptr\u201d to perform the gradient-based nonlinear optimisation with bounds and equality constraints. Augmented Lagrangian (AUGLAG) is used for global optimisation and Sequential Least Square Quadratic Programming (SLSQP) for local optimisation. For algorithmic details please see ",(0,o.kt)("a",{parentName:"p",href:"https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#slsqp"},"nloptr\u2019s documentation"),"."),(0,o.kt)("p",null,"Below is an example allocator onepager:"),(0,o.kt)("img",{alt:"budget allocator chart",src:(0,r.Z)("/img/optimizer_new.png")}),(0,o.kt)("p",null,"For the interpretation of the budget allocation onepager, please refer to the\nfollowing deepdive articles:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},'"The convergence of marginal ROAS in the budget allocation in Robyn"'),": ",(0,o.kt)("a",{parentName:"li",href:"https://medium.com/@gufengzhou/the-convergence-of-marginal-roas-in-the-budget-allocation-in-robyn-5d407aebf021"},"here")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},'"Hitting ROAS target using Robyn\u2019s budget allocator"'),": ",(0,o.kt)("a",{parentName:"li",href:"https://medium.com/@gufengzhou/hitting-roas-target-using-robyns-budget-allocator-274ace3add4f"},"here"))),(0,o.kt)("hr",null),(0,o.kt)("h2",{id:"model-refresh"},"Model refresh"),(0,o.kt)("p",null,"Conventionally, the model refresh cycle of MMM is bi-annual or even longer. The time delay leads to limited actionability of MMM insights. Robyn's model refresh feature aims to solve this challenge and enables model refresh and reporting as frequently as the data allows. It also enables MMM to be a continuous reporting tool for actionable and timely decision-making that could feed your reporting or BI tools."),(0,o.kt)("p",null,"Conceptually, the model refresh function ",(0,o.kt)("inlineCode",{parentName:"p"},"robyn_refresh()")," builds on top of an selected initial model, updates its hyperparameter with a narrower range that's proportional to the new data / total data ratio and picts an winning model based on the combination of objective functions. Note that model refreshing comes with certain common expectations. When refreshing with a shorter period of new data, it's expected:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"that the refreshed model has stable baseline compared to initial model"),(0,o.kt)("li",{parentName:"ul"},"that the refreshed model reflects the media spend changes in the new period")),(0,o.kt)("p",null,"Robyn uses modified objective functions to achieve the above. To be precise, the NRMSE only accounts for the new period while refreshing, because the main goal of refreshing is to better describe the new period. The DECOMP.RSSD is modified to drive only effect share closer to the refresh spend share, while it also accounts for the similarity between new and old baseline."),(0,o.kt)("p",null,"The example below shows the model refreshing mechanism for 5 different periods of time based in an initial window covering most of 2017 and 2018:"),(0,o.kt)("img",{alt:"pareto chart 2",src:(0,r.Z)("/img/refresh-window.png")}),(0,o.kt)("p",null,"The second refresh chart shows the model decomposition across initial and various refresh models:"),(0,o.kt)("img",{alt:"pareto chart 2",src:(0,r.Z)("/img/refresh-reporting.png")}),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note that this feature is not yet tested thoroughly")," and might provide instable refresh results."))}f.isMDXComponent=!0}}]);